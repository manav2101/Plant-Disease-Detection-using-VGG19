{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":150545,"sourceType":"datasetVersion","datasetId":70909},{"sourceId":7065277,"sourceType":"datasetVersion","datasetId":4068096},{"sourceId":7068599,"sourceType":"datasetVersion","datasetId":4070465}],"dockerImageVersionId":12836,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/manav2101/plant-disease-detection?scriptVersionId=161586375\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Convulational Neural Network","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport cv2\nfrom os import listdir\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras import backend as Kw \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nEPOCHS = 25\nINIT_LR = 1e-3\nBS = 32\ndefault_image_size = tuple((256, 256))\nimage_size = 0\ndirectory_root = '/kaggle/input/plantdisease'\nwidth=256\nheight=256\ndepth=3\n\ndef convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None :\n            image = cv2.resize(image, default_image_size)   \n            return img_to_array(image)\n        else :\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None\n\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir :\n        # remove .DS_Store from list\n        if directory == \".DS_Store\" :\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir :\n        plant_disease_folder_list = listdir(f\"{directory_root}/{plant_folder}\")\n        \n        for disease_folder in plant_disease_folder_list :\n            # remove .DS_Store from list\n            if disease_folder == \".DS_Store\" :\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}/{plant_folder}/{plant_disease_folder}/\")\n                \n            for single_plant_disease_image in plant_disease_image_list :\n                if single_plant_disease_image == \".DS_Store\" :\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}/{plant_folder}/{plant_disease_folder}/{image}\"\n                if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed\")  \nexcept Exception as e:\n    print(f\"Error : {e}\")\n\nimage_size = len(image_list)\n\nlabel_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\npickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\nn_classes = len(label_binarizer.classes_)\n\nprint(label_binarizer.classes_)\n\nnp_image_list = np.array(image_list, dtype=np.float16) / 225.0\n\nprint(\"[INFO] Spliting data to train, test\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) \n\naug = ImageDataGenerator(\n    rotation_range=25, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2, \n    zoom_range=0.2,horizontal_flip=True, \n    fill_mode=\"nearest\")\n\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\nmodel.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes, activation='softmax')) \nmodel.summary()\n\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n# distribution\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n# train the network\nprint(\"[INFO] training network...\")\n\nhistory = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) // BS,\n    epochs=EPOCHS, verbose=1\n    )\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()\n\nprint(\"[INFO] Calculating model accuracy\")\nscores = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {scores[1]*100}\")\n\n# Calculate Mean Squared Error (MSE)\ny_pred = model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error (MSE): {mse}\")\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Calculate Confusion Matrix\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\nconfusion_mat = confusion_matrix(y_true_classes, y_pred_classes)\n\ny_pred_bin = (y_pred > 0.5).astype(int)\ny_test_bin = (y_test > 0.5).astype(int)\n\nprecision = precision_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"Precision: {precision}\")\n\nrecall = recall_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"Recall: {recall}\")\n\nf1 = f1_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"F1 Score: {f1}\")\n\n# Plot Confusion Matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mat, annot=True, fmt='g', cmap='Blues', \n            xticklabels=label_binarizer.classes_, \n            yticklabels=label_binarizer.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# save the model to disk\nprint(\"[INFO] Saving model...\")\npickle.dump(model,open('cnn_model.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2024-01-12T14:40:05.902761Z","iopub.execute_input":"2024-01-12T14:40:05.903119Z","iopub.status.idle":"2024-01-12T14:40:44.339207Z","shell.execute_reply.started":"2024-01-12T14:40:05.90306Z","shell.execute_reply":"2024-01-12T14:40:44.337987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport cv2\nfrom os import listdir\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nEPOCHS = 25\nINIT_LR = 1e-3\nBS = 32\ndefault_image_size = tuple((256, 256))\nimage_size = 0\ndirectory_root = '/kaggle/input/plantdisease'\nwidth=256\nheight=256\ndepth=3\n\ndef convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None :\n            image = cv2.resize(image, default_image_size)   \n            return img_to_array(image)\n        else :\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None\n\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir :\n        # remove .DS_Store from list\n        if directory == \".DS_Store\" :\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir :\n        plant_disease_folder_list = listdir(f\"{directory_root}/{plant_folder}\")\n        \n        for disease_folder in plant_disease_folder_list :\n            # remove .DS_Store from list\n            if disease_folder == \".DS_Store\" :\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}/{plant_folder}/{plant_disease_folder}/\")\n                \n            for single_plant_disease_image in plant_disease_image_list :\n                if single_plant_disease_image == \".DS_Store\" :\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}/{plant_folder}/{plant_disease_folder}/{image}\"\n                if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed\")  \nexcept Exception as e:\n    print(f\"Error : {e}\")\n\nimage_size = len(image_list)\n\nlabel_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\npickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\nn_classes = len(label_binarizer.classes_)\n\nprint(label_binarizer.classes_)\n\nnp_image_list = np.array(image_list, dtype=np.float16) / 225.0\n\nprint(\"[INFO] Spliting data to train, test\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) \n\naug = ImageDataGenerator(\n    rotation_range=25, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2,\n    zoom_range=0.2, horizontal_flip=True,\n    fill_mode=\"nearest\")\n\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\nmodel.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes, activation='softmax')) \nmodel.summary()\n\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n# distribution\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n# train the network\nprint(\"[INFO] training network...\")\n\nhistory = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) // BS,\n    epochs=EPOCHS, verbose=1\n)\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()\n\nprint(\"[INFO] Calculating model accuracy\")\nscores = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {scores[1]*100}\")\n\n# Calculate Mean Squared Error (MSE)\ny_pred = model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error (MSE): {mse}\")\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Calculate Confusion Matrix\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\nconfusion_mat = confusion_matrix(y_true_classes, y_pred_classes)\n\ny_pred_bin = (y_pred > 0.5).astype(int)\ny_test_bin = (y_test > 0.5).astype(int)\n\nprecision = precision_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"Precision: {precision}\")\n\nrecall = recall_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"Recall: {recall}\")\n\nf1 = f1_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"F1 Score: {f1}\")\n\n#Plot Confusion Matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mat, annot=True, fmt='g', cmap='Blues', \n            xticklabels=label_binarizer.classes_, \n            yticklabels=label_binarizer.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n\n# save the model to disk\nprint(\"[INFO] Saving model...\")\nmodel.save('your_trained_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:24:37.654641Z","iopub.execute_input":"2024-01-08T05:24:37.654974Z","iopub.status.idle":"2024-01-08T05:37:34.197063Z","shell.execute_reply.started":"2024-01-08T05:24:37.654918Z","shell.execute_reply":"2024-01-08T05:37:34.19629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VGG19 Deep learning","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport cv2\nfrom os import listdir\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras.applications import VGG19\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nEPOCHS = 25\nINIT_LR = 1e-3\nBS = 32\ndefault_image_size = tuple((256, 256))\nimage_size = 0\ndirectory_root = '/kaggle/input/plantdisease'\nwidth=256\nheight=256\ndepth=3\n\ndef convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None :\n            image = cv2.resize(image, default_image_size)   \n            return img_to_array(image)\n        else :\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None\n\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir :\n        # remove .DS_Store from list\n        if directory == \".DS_Store\" :\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir :\n        plant_disease_folder_list = listdir(f\"{directory_root}/{plant_folder}\")\n        \n        for disease_folder in plant_disease_folder_list :\n            # remove .DS_Store from list\n            if disease_folder == \".DS_Store\" :\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}/{plant_folder}/{plant_disease_folder}/\")\n                \n            for single_plant_disease_image in plant_disease_image_list :\n                if single_plant_disease_image == \".DS_Store\" :\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}/{plant_folder}/{plant_disease_folder}/{image}\"\n                if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed\")  \nexcept Exception as e:\n    print(f\"Error : {e}\")\n\nimage_size = len(image_list)\n\nlabel_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\npickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\nn_classes = len(label_binarizer.classes_)\n\nprint(label_binarizer.classes_)\n\nnp_image_list = np.array(image_list, dtype=np.float16) / 225.0\n\nprint(\"[INFO] Spliting data to train, test\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) \n\naug = ImageDataGenerator(\n    rotation_range=25, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2,\n    zoom_range=0.2, horizontal_flip=True,\n    fill_mode=\"nearest\")\n\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\n\n\nbase_model = VGG19(weights='/kaggle/input/vgg-19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(256, 256, 3))\n\n# Build a new model on top of the pre-trained VGG19\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_classes, activation='softmax'))\n\n# Freeze the weights of the pre-trained layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel.summary()\n\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n# distribution\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n# train the network\nprint(\"[INFO] training network...\")\n\nhistory = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) // BS,\n    epochs=EPOCHS, verbose=1\n)\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\n#Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accurarcy')\nplt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\nplt.title('Training and Validation accurarcy')\nplt.legend()\n\nplt.figure()\n#Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()\n\nprint(\"[INFO] Calculating model accuracy\")\nscores = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {scores[1]*100}\")\n\n# Calculate Mean Squared Error (MSE)\ny_pred = model.predict(x_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error (MSE): {mse}\")\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = np.sqrt(mse)\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# Calculate Confusion Matrix\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\nconfusion_mat = confusion_matrix(y_true_classes, y_pred_classes)\n\ny_pred_bin = (y_pred > 0.5).astype(int) #data regression form convert classification form\ny_test_bin = (y_test > 0.5).astype(int) #date regression convert classification form\n\nprecision = precision_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"Precision: {precision}\")\n\nrecall = recall_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"Recall: {recall}\")\n\nf1 = f1_score(y_test_bin, y_pred_bin, average='weighted')\nprint(f\"F1 Score: {f1}\")\n\n#Plot Confusion Matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mat, annot=True, fmt='g', cmap='Blues', \n            xticklabels=label_binarizer.classes_, \n            yticklabels=label_binarizer.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# Dataset distribution\nplt.figure(figsize=(8, 8))\nplt.pie(np.bincount(np.argmax(image_labels, axis=1)), labels=label_binarizer.classes_, autopct='%1.1f%%')\nplt.title('Dataset Distribution')\nplt.show()\n\n# Convert images to uint8 data type before processing\nx_train_uint8 = (np.array(x_train) * 255).astype(np.uint8)\nhsv_images = [cv2.cvtColor(img, cv2.COLOR_RGB2HSV) for img in x_train_uint8]\n\n# Plot original and HSV images\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\naxes[0].imshow(x_train[0])\naxes[0].set_title('Original RGB Image')\naxes[0].axis('off')\naxes[1].imshow(cv2.cvtColor(hsv_images[0], cv2.COLOR_HSV2RGB))  # Convert HSV to RGB for display\naxes[1].set_title('HSV Image')\naxes[1].axis('off')\nplt.show()\n\n# Plot RGB channels\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, channel in enumerate(['Red', 'Green', 'Blue']):\n    axes[i].hist(x_train_uint8[0][:, :, i].ravel(), bins=256, color=channel.lower(), alpha=0.7)\n    axes[i].set_title(f'{channel} Channel Histogram')\nplt.show()\n\n# Print leaf images with segmentation\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\nfor i in range(4):\n    axes[0, i].imshow(x_train[i])\n    axes[0, i].set_title('Original RGB Image')\n    axes[0, i].axis('off')\n    axes[1, i].imshow(cv2.cvtColor(hsv_images[i], cv2.COLOR_HSV2RGB))  # Convert HSV to RGB for display\n    axes[1, i].set_title('HSV Image')\n    axes[1, i].axis('off')\nplt.show()\n\n# save the model to disk\nprint(\"[INFO] Saving model...\")\nmodel.save('your_trained_model.h5 ')","metadata":{"execution":{"iopub.status.busy":"2024-02-02T10:43:12.83123Z","iopub.execute_input":"2024-02-02T10:43:12.831646Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[INFO] Loading images ...\n[INFO] Processing Pepper__bell___Bacterial_spot ...\n[INFO] Processing Potato___healthy ...\n[INFO] Processing Tomato_Leaf_Mold ...\n[INFO] Processing Tomato__Tomato_YellowLeaf__Curl_Virus ...\n[INFO] Processing Tomato_Bacterial_spot ...\n[INFO] Processing Tomato_Septoria_leaf_spot ...\n[INFO] Processing Tomato_healthy ...\n[INFO] Processing Tomato_Spider_mites_Two_spotted_spider_mite ...\n[INFO] Processing Tomato_Early_blight ...\n[INFO] Processing Tomato__Target_Spot ...\n[INFO] Processing Pepper__bell___healthy ...\n[INFO] Processing Potato___Late_blight ...\n[INFO] Processing Tomato_Late_blight ...\n[INFO] Processing Potato___Early_blight ...\n[INFO] Processing Tomato__Tomato_mosaic_virus ...\n[INFO] Processing PlantVillage ...\n[INFO] Image loading completed\n['Pepper__bell___Bacterial_spot' 'Pepper__bell___healthy'\n 'Potato___Early_blight' 'Potato___Late_blight' 'Potato___healthy'\n 'Tomato_Bacterial_spot' 'Tomato_Early_blight' 'Tomato_Late_blight'\n 'Tomato_Leaf_Mold' 'Tomato_Septoria_leaf_spot'\n 'Tomato_Spider_mites_Two_spotted_spider_mite' 'Tomato__Target_Spot'\n 'Tomato__Tomato_YellowLeaf__Curl_Virus' 'Tomato__Tomato_mosaic_virus'\n 'Tomato_healthy']\n[INFO] Spliting data to train, test\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg19 (Model)                (None, 8, 8, 512)         20024384  \n_________________________________________________________________\nglobal_average_pooling2d_6 ( (None, 512)               0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 512)               262656    \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 512)               2048      \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 15)                7695      \n=================================================================\nTotal params: 20,296,783\nTrainable params: 271,375\nNon-trainable params: 20,025,408\n_________________________________________________________________\n[INFO] training network...\nEpoch 1/25\n28/73 [==========>...................] - ETA: 14:38 - loss: 2.1067 - acc: 0.3326","output_type":"stream"}]},{"cell_type":"code","source":"!python setup.py install\n","metadata":{"execution":{"iopub.status.busy":"2024-01-14T20:37:18.322205Z","iopub.execute_input":"2024-01-14T20:37:18.322522Z","iopub.status.idle":"2024-01-14T20:37:21.425529Z","shell.execute_reply.started":"2024-01-14T20:37:18.322473Z","shell.execute_reply":"2024-01-14T20:37:21.424735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning + Genetic Algorithm","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport cv2\nfrom os import listdir\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras.applications import VGG19\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pykopt import differential_evolution\n\nEPOCHS = 25\nINIT_LR = 1e-3\nBS = 32\ndefault_image_size = tuple((256, 256))\nimage_size = 0\ndirectory_root = '/kaggle/input/plantdisease'\nwidth = 256\nheight = 256\ndepth = 3\n\ndef convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None:\n            image = cv2.resize(image, default_image_size)\n            return img_to_array(image)\n        else:\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None\n\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir:\n        if directory == \".DS_Store\":\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir:\n        plant_disease_folder_list = listdir(f\"{directory_root}/{plant_folder}\")\n\n        for disease_folder in plant_disease_folder_list:\n            if disease_folder == \".DS_Store\":\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}/{plant_folder}/{plant_disease_folder}/\")\n\n            for single_plant_disease_image in plant_disease_image_list:\n                if single_plant_disease_image == \".DS_Store\":\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}/{plant_folder}/{plant_disease_folder}/{image}\"\n                if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed\")\nexcept Exception as e:\n    print(f\"Error : {e}\")\n\nimage_size = len(image_list)\n\nlabel_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\npickle.dump(label_binarizer, open('label_transform.pkl', 'wb'))\nn_classes = len(label_binarizer.classes_)\n\nprint(label_binarizer.classes_)\n\nnp_image_list = np.array(image_list, dtype=np.float16) / 225.0\n\nprint(\"[INFO] Splitting data into train, test\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state=42)\n\naug = ImageDataGenerator(\n    rotation_range=25, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2,\n    zoom_range=0.2, horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\ndef fitness(params):\n    dropout_rate, learning_rate = params\n    print(f\"[INFO] Running with Dropout: {dropout_rate}, Learning Rate: {learning_rate}\")\n\n    # Build the model\n    base_model = VGG19(weights='/kaggle/input/vgg-19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                      include_top=False, input_shape=(256, 256, 3))\n\n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(512, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(n_classes, activation='softmax'))\n\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    opt = Adam(lr=learning_rate, decay=learning_rate / EPOCHS)\n    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\n    history = model.fit_generator(\n        aug.flow(x_train, y_train, batch_size=BS),\n        validation_data=(x_test, y_test),\n        steps_per_epoch=len(x_train) // BS,\n        epochs=EPOCHS,\n        verbose=1\n    )\n\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    return -val_acc[-1]  \n\nvarbound = np.array([[0.1, 0.5],\n                     [1e-5, 1e-2]]) \n\nresult = differential_evolution(fitness, varbound, maxiter=100, popsize=10, disp=True)\n\nbest_params = result.x\nbest_dropout, best_learning_rate = best_params\n\nprint(f\"Best Dropout Rate: {best_dropout}\")\nprint(f\"Best Learning Rate: {best_learning_rate}\")\n\nfinal_model = Sequential()\nfinal_model.add(base_model)\nfinal_model.add(GlobalAveragePooling2D())\nfinal_model.add(Dense(512, activation='relu'))\nfinal_model.add(BatchNormalization())\nfinal_model.add(Dropout(best_dropout))\nfinal_model.add(Dense(n_classes, activation='softmax'))\n\nopt = Adam(lr=best_learning_rate, decay=best_learning_rate / EPOCHS)\nfinal_model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\nprint(\"[INFO] Training the final model with the best parameters...\")\nfinal_history = final_model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) // BS,\n    epochs=EPOCHS,\n    verbose=1\n)\n\nfinal_acc = final_history.history['accuracy']\nfinal_val_acc = final_history.history['val_accuracy']\nfinal_loss = final_history.history['loss']\nfinal_val_loss = final_history.history['val_loss']\nfinal_epochs = range(1, len(final_acc) + 1)\n\nplt.plot(final_epochs, final_acc, 'b', label='Training accuracy')\nplt.plot(final_epochs, final_val_acc, 'r', label='Validation accuracy')\nplt.title('Final Model Training and Validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(final_epochs, final_loss, 'b', label='Training loss')\nplt.plot(final_epochs, final_val_loss, 'r', label='Validation loss')\nplt.title('Final Model Training and Validation loss')\nplt.legend()\nplt.show()\n\nprint(\"[INFO] Calculating final model accuracy\")\nfinal_scores = final_model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {final_scores[1] * 100:.2f}%\")\n\nfinal_y_pred = final_model.predict(x_test)\nfinal_y_pred_classes = np.argmax(final_y_pred, axis=1)\nfinal_y_true_classes = np.argmax(y_test, axis=1)\nfinal_confusion_mat = confusion_matrix(final_y_true_classes, final_y_pred_classes)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(final_confusion_mat, annot=True, fmt='g', cmap='Blues',\n            xticklabels=label_binarizer.classes_,\n            yticklabels=label_binarizer.classes_)\nplt.title('Final Model Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint(\"[INFO] Saving final model...\")\nfinal_model.save('final_model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-01-14T20:23:47.696884Z","iopub.execute_input":"2024-01-14T20:23:47.69718Z","iopub.status.idle":"2024-01-14T20:23:49.268797Z","shell.execute_reply.started":"2024-01-14T20:23:47.697127Z","shell.execute_reply":"2024-01-14T20:23:49.267559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Radial Base Function using SVM","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport cv2\nfrom os import listdir\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras import backend as K\n\nEPOCHS = 25\nINIT_LR = 1e-3\nBS = 32\ndefault_image_size = tuple((256, 256))\nimage_size = 0\ndirectory_root = '/kaggle/input/plantdisease'\nwidth = 256\nheight = 256\ndepth = 3\n\n# Custom RBF layer\nclass RBFLayer(tf.keras.layers.Layer):\n    def __init__(self, units, gamma, **kwargs):\n        super(RBFLayer, self).__init__(**kwargs)\n        self.units = units\n        self.gamma = gamma\n\n    def build(self, input_shape):\n        self.centers = self.add_weight(name='centers',\n                                       shape=(self.units, input_shape[1]),\n                                       initializer='uniform',\n                                       trainable=True)\n        super(RBFLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        diff = K.expand_dims(inputs) - self.centers\n        l2 = K.sum(K.pow(diff, 2), axis=2)\n        res = K.exp(-1 * self.gamma * l2)\n        return res\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)\n\ndef convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None:\n            image = cv2.resize(image, default_image_size)\n            return img_to_array(image)\n        else:\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None\n\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir:\n        # remove .DS_Store from the list\n        if directory == \".DS_Store\":\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir:\n        plant_disease_folder_list = listdir(f\"{directory_root}/{plant_folder}\")\n\n        for disease_folder in plant_disease_folder_list:\n            # remove .DS_Store from the list\n            if disease_folder == \".DS_Store\":\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}/{plant_folder}/{plant_disease_folder}/\")\n\n            for single_plant_disease_image in plant_disease_image_list:\n                if single_plant_disease_image == \".DS_Store\":\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}/{plant_folder}/{plant_disease_folder}/{image}\"\n                if image_directory.endswith(\".jpg\") or image_directory.endswith(\".JPG\"):\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed\")\nexcept Exception as e:\n    print(f\"Error : {e}\")\n\nimage_size = len(image_list)\n\nlabel_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\npickle.dump(label_binarizer, open('label_transform.pkl', 'wb'))\nn_classes = len(label_binarizer.classes_)\n\nprint(label_binarizer.classes_)\n\nnp_image_list = np.array(image_list, dtype=np.float16) / 225.0\n\nprint(\"[INFO] Splitting data into train, test\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state=42)\n\naug = ImageDataGenerator(\n    rotation_range=25, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2,\n    zoom_range=0.2, horizontal_flip=True,\n    fill_mode=\"nearest\")\n\nmodel = Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    inputShape = (depth, height, width)\n    chanDim = 1\n\n# Add a Convolutional layer\nmodel.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\n\n# Add another Convolutional layer\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flatten the feature maps\nmodel.add(Flatten())\n\n# Add the custom RBF layer\nmodel.add(RBFLayer(units=512, gamma=0.5))\n\n# Add a Dense layer\nmodel.add(Dense(512))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n# Add the output layer\nmodel.add(Dense(n_classes))\nmodel.add(Activation(\"softmax\"))\n\n# Compile the model\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\n# Train the model\nprint(\"[INFO] Training network...\")\nhistory = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size=BS),\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) // BS,\n    epochs=EPOCHS,\n    verbose=1\n)\n\n# Plot the training history\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\n# Train and validation accuracy\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.legend()\n\nplt.figure()\n# Train and validation loss\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()\n\n# Evaluate the model\nprint(\"[INFO] Calculating model accuracy\")\nscores = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {scores[1] * 100}\")\n\n# Calculate Confusion Matrix\ny_pred = model.predict(x_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true_classes = np.argmax(y_test, axis=1)\nconfusion_mat = confusion_matrix(y_true_classes, y_pred_classes)\n\n# Plot Confusion Matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mat, annot=True, fmt='g', cmap='Blues',\n            xticklabels=label_binarizer.classes_,\n            yticklabels=label_binarizer.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# Save the model to disk\nprint(\"[INFO] Saving model...\")\nmodel.save('your_trained_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T22:10:07.47907Z","iopub.execute_input":"2023-11-27T22:10:07.479398Z","iopub.status.idle":"2023-11-27T22:10:39.637276Z","shell.execute_reply.started":"2023-11-27T22:10:07.479348Z","shell.execute_reply":"2023-11-27T22:10:39.635925Z"},"trusted":true},"execution_count":null,"outputs":[]}]}